apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: training
spec:
  replicatedJobs:
  - name: workers
    template:
      spec:
        parallelism: 1
        completions: 1
        backoffLimit: 0
        template:
          #metadata:
          #   annotations:
          #     gke-gcsfuse/volumes: "true"
          spec:
            #schedulingGates:
            #- name: "gke.io/topology-aware-auto-scheduling-<USER>-<JOB_NAME>"
            hostNetwork: true
            dnsPolicy: ClusterFirstWithHostNet
            nodeSelector:
               cloud.google.com/gke-accelerator: nvidia-l4
            #serviceAccountName: training
            volumes:
            - name: dshm
              emptyDir:
                medium: Memory
            containers:
           
            - name: pytorch
              image: us-east1-docker.pkg.dev/rick-vertex-ai/gke-llm/mlflow-wandb:latest
              imagePullPolicy: Always
              # image: gcr.io/k8s-staging-jobset/pytorch-mnist:latest
              ports:
              - containerPort: 3389
              env:
              - name: MASTER_ADDR
                value: "training-workers-0-0.training"
              - name: MASTER_PORT
                value: "3389"
              - name: NODE_COUNT
                value: "1"
              - name: NODE_RANK
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
              # Force python to not buffer output and write directly to stdout, so we can view training logs via `kubectl logs`.
              - name: PYTHONUNBUFFERED
                value: "0"    
              - name: WANDB_API_KEY
                valueFrom:
                  secretKeyRef:
                    name: wandbapikey
                    key: WANDB_API_KEY                
              #- name: PYTHON_SCRIPT
              #  value: "experiments/diffusion/diffusion.py --config=<CONFIG_FILE> --trainer.num_nodes=<NODE_COUNT> --trainer.logger.name=<JOB_NAME> --trainer.devices=auto <CONFIG_OPTIONS> fit"
             
              securityContext:
                privileged: true    
              command:  
               - bash
               - -xc
               - |
                 sleep 60000 
                 # mlflow run . -P hydra_options="etl.sample='sample2.csv'"
              resources:
                requests:
                  cpu: "2"
                  memory: "25Gi"
                  ephemeral-storage: "25Gi"
                  nvidia.com/gpu: 2
                limits:
                  # cpu: "16"
                  # memory: "30Gi"
                  # ephemeral-storage: "30Gi"
                  nvidia.com/gpu: 2

              volumeMounts:
               - mountPath: /dev/shm
                 name: dshm      